{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate --quiet\n!pip install emoji --quiet","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:27:42.660188Z","iopub.execute_input":"2023-08-15T16:27:42.660552Z","iopub.status.idle":"2023-08-15T16:28:09.424814Z","shell.execute_reply.started":"2023-08-15T16:27:42.660523Z","shell.execute_reply":"2023-08-15T16:28:09.423542Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom emoji import demojize\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport os, re, random, datasets, evaluate\nfrom sklearn.model_selection import train_test_split\n\npd.set_option('display.max_colwidth', None)\nfrom transformers import AutoTokenizer, TFAutoModel, DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-15T16:28:09.428011Z","iopub.execute_input":"2023-08-15T16:28:09.428420Z","iopub.status.idle":"2023-08-15T16:28:24.412155Z","shell.execute_reply.started":"2023-08-15T16:28:09.428379Z","shell.execute_reply":"2023-08-15T16:28:24.411163Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:28:24.413342Z","iopub.execute_input":"2023-08-15T16:28:24.413687Z","iopub.status.idle":"2023-08-15T16:28:24.485552Z","shell.execute_reply.started":"2023-08-15T16:28:24.413655Z","shell.execute_reply":"2023-08-15T16:28:24.484369Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess the Data Sets","metadata":{}},{"cell_type":"markdown","source":"In the past few notebooks, I established a pre-processing pipeline where I: (1) identify misclassified tweets (duplicate tweets whose labels are not identical), (2) concatenate the substance of the location column with that of the text column, and (3) clean the tweets following the set of pre-processing steps that VinAI used prior to training the BERTweet model. You can read more about the pre-processing steps by taking a look at one of my previous notebooks [here](https://www.kaggle.com/code/l048596/disaster-tweets-bertweet-pytorch-ii-82-62?kernelSessionId=139348416). ","metadata":{}},{"cell_type":"code","source":"duplicates = train[train.duplicated('text')]\nproblematic_duplicates = []\n\nfor i in range(duplicates.text.nunique()):\n    duplicate_subset = train[train.text == duplicates.text.unique()[i]]\n    if len(duplicate_subset) > 1 and duplicate_subset.target.nunique() == 2:\n        problematic_duplicates.append(i)\n        \ntarget_list = [0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0]\n\nfor problematic_index in range(len(problematic_duplicates)): \n    train.target = np.where(train.text == duplicates.text.unique()[problematic_index], \n                            target_list[problematic_index], train.target)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:28:26.891155Z","iopub.execute_input":"2023-08-15T16:28:26.892041Z","iopub.status.idle":"2023-08-15T16:28:27.059037Z","shell.execute_reply.started":"2023-08-15T16:28:26.892008Z","shell.execute_reply":"2023-08-15T16:28:27.057836Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def clean_tweets(text):\n    \n    text = text.lower()\n    \n    text = text.replace(\"n't\", \" n't \")\n    text = text.replace(\"n 't\", \" n't \")\n    text = text.replace(\"ca n't\", \"can't\")\n    text = text.replace(\"ai n't\", \"ain't\")\n    \n    text = text.replace(\"'m\", \" 'm \")\n    text = text.replace(\"'re\", \" 're \")\n    text = text.replace(\"'s\", \" 's \")\n    text = text.replace(\"'ll\", \" 'll \")\n    text = text.replace(\"'d\", \" 'd \")\n    text = text.replace(\"'ve\", \" 've \")\n    text = text.replace(\"\\n\", \" \")\n    \n    text = text.replace(\" p . m .\", \" p.m.\")\n    text = text.replace(\" p . m \", \" p.m \")\n    text = text.replace(\" a . m .\", \" a.m.\")\n    text = text.replace(\" a . m \", \" a.m \")\n    \n    token_list = text.split(' ')\n    \n    token_list = [re.sub('#', '', x) for x in token_list]\n    token_list = [re.sub(r'@\\S+', '@USER', x) for x in token_list]\n    token_list = [re.sub(r'http\\S+', 'HTTPURL', x) for x in token_list]\n    token_list = [re.sub(r'www\\S+', 'HTTPURL', x) for x in token_list]\n    token_list = [demojize(x) if len(x) == 1 else x for x in token_list]\n    \n    return(\" \".join(token_list))","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:28:28.953846Z","iopub.execute_input":"2023-08-15T16:28:28.954238Z","iopub.status.idle":"2023-08-15T16:28:28.964179Z","shell.execute_reply.started":"2023-08-15T16:28:28.954207Z","shell.execute_reply":"2023-08-15T16:28:28.963190Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train.location = train.location.replace(np.nan, '', regex = True)\ntest_df.location = test_df.location.replace(np.nan, '', regex = True)\n\ntrain.text = train.text + \". \" + train.location + \".\"\ntest_df.text = test_df.text + \". \" + test_df.location + \".\"\n\ntrain.text = train.text.apply(lambda x: clean_tweets(x))\ntest_df.text = test_df.text.apply(lambda x: clean_tweets(x))","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:28:31.509965Z","iopub.execute_input":"2023-08-15T16:28:31.510676Z","iopub.status.idle":"2023-08-15T16:28:32.999780Z","shell.execute_reply.started":"2023-08-15T16:28:31.510642Z","shell.execute_reply":"2023-08-15T16:28:32.998729Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train = train.groupby('target').sample(np.min(train.target.value_counts().to_list()), random_state = 1048596)\ntrain_df, val_df = np.split(train.sample(frac = 1), [int(0.85 * len(train))])","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:28:34.728456Z","iopub.execute_input":"2023-08-15T16:28:34.728896Z","iopub.status.idle":"2023-08-15T16:28:34.745820Z","shell.execute_reply.started":"2023-08-15T16:28:34.728858Z","shell.execute_reply":"2023-08-15T16:28:34.744882Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Load Pre-trained Model for Tokenization","metadata":{}},{"cell_type":"markdown","source":"Initially, I had planned to fine-tune the BERTweet model and add one or two Dense and Dropout layers so that we can enhance the expressive power of the model that we had in the previous [notebook](https://www.kaggle.com/code/l048596/disaster-tweets-bertweet-pytorch-ii-82-62?kernelSessionId=139348416). However, instead of doing that, I decided to learn to fine-tune another Hugging Face model just using TensorFlow, in this case DistilBERT, and figure out a standard procedure that I can use to add additional Keras layers on top of the fine-tuned models on Hugging Face for text classification. ","metadata":{}},{"cell_type":"code","source":"model_name = 'distilbert-base-uncased'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, \n                                          normalization = True,\n                                          use_fast = False,\n                                          add_special_tokens = True,\n                                          pad_to_max_length = True, \n                                          return_attention_mask = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:28:37.571026Z","iopub.execute_input":"2023-08-15T16:28:37.571424Z","iopub.status.idle":"2023-08-15T16:28:38.248156Z","shell.execute_reply.started":"2023-08-15T16:28:37.571393Z","shell.execute_reply":"2023-08-15T16:28:38.247132Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b2a6cdc9c0c468d8b2e3ea8970ea75b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d9588d6d17f4fc28c868b9776479614"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"543b865dd6db462ea6e0e0cfaef02e94"}},"metadata":{}}]},{"cell_type":"code","source":"train_tokens = tokenizer(train_df.text.to_list(),\n                         padding = \"max_length\",\n                         truncation = True).data\n\nval_tokens = tokenizer(val_df.text.to_list(),\n                       padding = \"max_length\",\n                       truncation = True).data","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:28:40.490685Z","iopub.execute_input":"2023-08-15T16:28:40.491824Z","iopub.status.idle":"2023-08-15T16:28:47.498740Z","shell.execute_reply.started":"2023-08-15T16:28:40.491785Z","shell.execute_reply":"2023-08-15T16:28:47.497744Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def extract_features(tokens, labels, batch_size = 16): # Note that batch size of 64 willr esult in GPU OOM error\n    features = {x: tokens[x] for x in tokenizer.model_input_names}\n    features = tf.data.Dataset.from_tensor_slices((features, labels))\n    return features.shuffle(len(labels)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\ntrain_features = extract_features(train_tokens, train_df.target)\nval_features = extract_features(val_tokens, val_df.target)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:28:50.890473Z","iopub.execute_input":"2023-08-15T16:28:50.890852Z","iopub.status.idle":"2023-08-15T16:29:18.094877Z","shell.execute_reply.started":"2023-08-15T16:28:50.890823Z","shell.execute_reply":"2023-08-15T16:29:18.093883Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"I tried using a number of different pre-trained models on Hugging Face and came to realize that one has to modify certain parts of the code to make things work (if you simply change the *model_name* to some other model on Hugging Face, you will most likely get errors. So, before we proceed onto using the DistilBERT model here, let's take a look at the input and output of the DistilBERT model to determine which part of the code has to be modified so that we can further fine-tune Hugging Face models:","metadata":{}},{"cell_type":"code","source":"bert_model = TFAutoModel.from_pretrained(model_name)\ntext = [\"Replace me by any text you'd like.\", \"My name is Messi Lee\"]\n\nencoded_input = tokenizer(text, \n                          padding = \"max_length\", \n                          truncation = True,\n                          return_tensors='tf')\n\noutput = bert_model([encoded_input['input_ids'], encoded_input['attention_mask']])\noutput","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:29:19.793353Z","iopub.execute_input":"2023-08-15T16:29:19.793912Z","iopub.status.idle":"2023-08-15T16:29:25.379087Z","shell.execute_reply.started":"2023-08-15T16:29:19.793869Z","shell.execute_reply":"2023-08-15T16:29:25.378115Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f12f0420c86545d1bb878b6c8d2c9725"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFDistilBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TFBaseModelOutput(last_hidden_state=<tf.Tensor: shape=(2, 512, 768), dtype=float32, numpy=\narray([[[ 4.41138633e-04, -2.62405723e-01, -1.01915449e-01, ...,\n         -6.27640188e-02,  2.75840908e-01,  3.70140642e-01],\n        [ 7.22330213e-01,  1.64490327e-01,  4.00247574e-01, ...,\n          1.91608697e-01,  4.04579461e-01, -5.80942333e-02],\n        [ 2.81979889e-01, -1.74299002e-01,  3.90757024e-02, ...,\n          2.76808701e-02,  1.18860215e-01,  9.14387286e-01],\n        ...,\n        [ 1.63950697e-01,  4.58683260e-02,  9.72631425e-02, ...,\n         -1.33402888e-02, -1.13538861e-01, -3.34521234e-02],\n        [ 2.25677252e-01,  5.23837283e-02,  2.04405099e-01, ...,\n         -1.12724975e-02, -1.48728728e-01, -5.23389578e-02],\n        [ 1.87630862e-01, -1.89079866e-01,  3.37699950e-02, ...,\n          1.79747820e-01, -2.19924212e-01, -1.39652103e-01]],\n\n       [[-6.15856461e-02, -1.84155684e-02,  6.84209168e-02, ...,\n          3.34893987e-02,  2.31654853e-01,  2.43140161e-01],\n        [ 2.97467321e-01, -1.25447005e-01, -3.71922910e-01, ...,\n         -1.27921969e-01,  4.44294393e-01,  3.40015054e-01],\n        [-2.47224227e-01, -1.51072722e-02, -2.58492231e-01, ...,\n         -2.96506464e-01,  2.85631299e-01, -3.00750732e-01],\n        ...,\n        [ 2.28416771e-01, -5.51248081e-02,  1.00132957e-01, ...,\n          3.74724269e-01, -1.65876687e-01,  5.92765771e-02],\n        [ 1.87027186e-01, -5.77315763e-02,  1.16892405e-01, ...,\n          3.30714285e-01, -1.22062877e-01,  2.37066001e-02],\n        [ 1.40198916e-01, -2.39253882e-02,  2.72366196e-01, ...,\n          2.96865404e-01, -1.13361411e-01, -1.75674260e-01]]],\n      dtype=float32)>, hidden_states=None, attentions=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"It is important to note that the last hidden state of the DistilBERT model is of shape (2, 512, 768). Here, 2 corresponds to the number of texts, 512 corresponds to the length of the tokenized input, and 768 corresonds to the dimension of the DistilBERT embedding. The value 512 is going to be used for the shape of the input layers in our model. Depending on the BERT model configuration, this value can vary (e.g., BERTweet model had the value of 128). Furthermore, we are going to take the very first (0th) element of the BERT output (instead of the second element for BERTweet model) and feed it into the Dropout layer. Also, I've come to notice that using a relatively big dropout value (we are going to try 0.7 here) is helpful for the generalizability of the model trained here: ","metadata":{}},{"cell_type":"code","source":"bert_model = TFAutoModel.from_pretrained(model_name)\n\ninput_ids = tf.keras.Input(shape=(512,), dtype = 'int32', name = 'input_ids')\nattention_masks = tf.keras.Input(shape=(512,), dtype ='int32', name = 'attention_mask')\n\noutput = bert_model([input_ids, attention_masks])[0]\noutput = tf.keras.layers.Dropout(0.7)(output)\noutput = tf.keras.layers.Flatten()(output)\noutput = tf.keras.layers.Dense(1, activation = 'sigmoid')(output)\n\nmodel = tf.keras.models.Model(inputs = [input_ids, attention_masks], outputs = output)\n\nmodel.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = 1e-5), \n              loss = tf.keras.losses.BinaryCrossentropy(), \n              metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:29:32.716089Z","iopub.execute_input":"2023-08-15T16:29:32.716544Z","iopub.status.idle":"2023-08-15T16:29:39.003740Z","shell.execute_reply.started":"2023-08-15T16:29:32.716492Z","shell.execute_reply":"2023-08-15T16:29:39.002766Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFDistilBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:29:48.941352Z","iopub.execute_input":"2023-08-15T16:29:48.941718Z","iopub.status.idle":"2023-08-15T16:29:48.980371Z","shell.execute_reply.started":"2023-08-15T16:29:48.941689Z","shell.execute_reply":"2023-08-15T16:29:48.979626Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_ids (InputLayer)         [(None, 512)]        0           []                               \n                                                                                                  \n attention_mask (InputLayer)    [(None, 512)]        0           []                               \n                                                                                                  \n tf_distil_bert_model_1 (TFDist  TFBaseModelOutput(l  66362880   ['input_ids[0][0]',              \n ilBertModel)                   ast_hidden_state=(N               'attention_mask[0][0]']         \n                                one, 512, 768),                                                   \n                                 hidden_states=None                                               \n                                , attentions=None)                                                \n                                                                                                  \n dropout_38 (Dropout)           (None, 512, 768)     0           ['tf_distil_bert_model_1[0][0]'] \n                                                                                                  \n flatten (Flatten)              (None, 393216)       0           ['dropout_38[0][0]']             \n                                                                                                  \n dense (Dense)                  (None, 1)            393217      ['flatten[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 66,756,097\nTrainable params: 66,756,097\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Finally, it helps to inspect the model using the **summary()** function as it shows the output shapes for all layers. As the ouput of the DistilBERT model is of shape (None, 512, 768), even after the output is fed into the Dropout layer and the Dense layer, the shape stays that way. However, for us to calculate the metric specified in this model (\"accuracy\"), the output of the final layer of this model has to be of the shape (None, 1). To do that, we introduce a Flatten layer right before the Dense layer. ","metadata":{}},{"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n                                                  patience = 2)\n\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath = 'model/best_performed_model',\n    save_weights_only = True,\n    save_best_only = True,\n    monitor = 'val_loss',\n    verbose = 1\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:29:52.257777Z","iopub.execute_input":"2023-08-15T16:29:52.258171Z","iopub.status.idle":"2023-08-15T16:29:52.264428Z","shell.execute_reply.started":"2023-08-15T16:29:52.258141Z","shell.execute_reply":"2023-08-15T16:29:52.263315Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model.fit(train_features, \n          validation_data = val_features,\n          epochs = 30, \n          callbacks = [early_stopping, model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:29:56.379270Z","iopub.execute_input":"2023-08-15T16:29:56.379649Z","iopub.status.idle":"2023-08-15T16:53:06.427205Z","shell.execute_reply.started":"2023-08-15T16:29:56.379621Z","shell.execute_reply":"2023-08-15T16:53:06.426122Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/30\n346/346 [==============================] - ETA: 0s - loss: 0.4780 - accuracy: 0.7733\nEpoch 1: val_loss improved from inf to 0.37805, saving model to model/best_performed_model\n346/346 [==============================] - 355s 1s/step - loss: 0.4780 - accuracy: 0.7733 - val_loss: 0.3780 - val_accuracy: 0.8362\nEpoch 2/30\n346/346 [==============================] - ETA: 0s - loss: 0.3713 - accuracy: 0.8412\nEpoch 2: val_loss improved from 0.37805 to 0.37747, saving model to model/best_performed_model\n346/346 [==============================] - 346s 1s/step - loss: 0.3713 - accuracy: 0.8412 - val_loss: 0.3775 - val_accuracy: 0.8434\nEpoch 3/30\n346/346 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.8750\nEpoch 3: val_loss did not improve from 0.37747\n346/346 [==============================] - 344s 995ms/step - loss: 0.3045 - accuracy: 0.8750 - val_loss: 0.3929 - val_accuracy: 0.8352\nEpoch 4/30\n346/346 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.9084\nEpoch 4: val_loss did not improve from 0.37747\n346/346 [==============================] - 344s 995ms/step - loss: 0.2282 - accuracy: 0.9084 - val_loss: 0.5771 - val_accuracy: 0.7881\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f7fa64010f0>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Prepare for Submission","metadata":{}},{"cell_type":"markdown","source":"I use the **model_checkpoint_callback** with *save_best_only* set to True. At the end of each epoch, when validation loss is calculated, the callback function checks if the validation loss at the end of the epoch is smallest, and if it is, it saves the weights of that model to the designated path. This is so that if the model overfits, the model weights that performed best with respect to validation loss can then be loaded for test set prediction. However, in my past notebooks, despite using the callback function, I had not loaded the best performing model prior to using the model to predict labels for the test data set. Moving forward, the following line of code will be called so that we can use the best performing model for prediction purposes. \n\nLet's load the model using the **load_weights()** function and then evaluate the model using the validation data set. If the best performing model was restored, validation accuracy should be 84.34% as it was at the second epoch: ","metadata":{}},{"cell_type":"code","source":"model.load_weights('model/best_performed_model')\nmodel.evaluate(val_features)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:53:27.028081Z","iopub.execute_input":"2023-08-15T16:53:27.028460Z","iopub.status.idle":"2023-08-15T16:53:47.299689Z","shell.execute_reply.started":"2023-08-15T16:53:27.028431Z","shell.execute_reply":"2023-08-15T16:53:47.298740Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"62/62 [==============================] - 19s 309ms/step - loss: 0.3775 - accuracy: 0.8434\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[0.3774731457233429, 0.8433981537818909]"},"metadata":{}}]},{"cell_type":"code","source":"test_token = tokenizer(test_df.text.tolist(), \n                       padding = \"max_length\", \n                       truncation = True,\n                       return_tensors='tf').data","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:53:50.620083Z","iopub.execute_input":"2023-08-15T16:53:50.621117Z","iopub.status.idle":"2023-08-15T16:53:53.317804Z","shell.execute_reply.started":"2023-08-15T16:53:50.621068Z","shell.execute_reply":"2023-08-15T16:53:53.316830Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_token)\npred = [(x > 0.5).astype(int)[0] for x in predictions]","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:53:54.650576Z","iopub.execute_input":"2023-08-15T16:53:54.650960Z","iopub.status.idle":"2023-08-15T16:55:18.290223Z","shell.execute_reply.started":"2023-08-15T16:53:54.650930Z","shell.execute_reply":"2023-08-15T16:55:18.288994Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"102/102 [==============================] - 67s 643ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.DataFrame(list(zip(test_df.id, pred)), columns = [\"id\", \"target\"])\nsubmission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T16:55:18.292142Z","iopub.execute_input":"2023-08-15T16:55:18.292608Z","iopub.status.idle":"2023-08-15T16:55:18.327372Z","shell.execute_reply.started":"2023-08-15T16:55:18.292571Z","shell.execute_reply":"2023-08-15T16:55:18.326421Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"The next notebook is going to be a slightly modified version of this notebook where I fine-tune DeBERTa in light of the discussions that have been taking place in many discussions pertaining to NLP competitions on Kaggle. Let's see if an equivalent model that fine-tunes DeBERTa performs significantly better than the one that fine-tunes DistilBERT and/or BERTweet. ","metadata":{}}]}