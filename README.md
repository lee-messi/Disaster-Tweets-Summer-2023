# About this Repository
These are the notebooks that I worked on for [the Natural Language Processing with Disaster Tweets competition on Kaggle](https://www.kaggle.com/competitions/nlp-getting-started). As of July 24th, 2023, the best performing model has the score of 0.79742 (Rank: 528). 

## Notebooks
- **Disaster Tweets Classifier using BERT**: We train three different models using BERT and achieve ~78% validation accuracy. This is not impressive compared to the performance of other models that are on the leaderboard (Best score: 0.79742; Rank: 528; Submission on: July 20th, 2023).

- **Disaster Tweets Classifier using LSTMs**: We train two additional models using Bi-directional LSTMs and achieve ~80% validation accuracy. This is a small improvement from the first set of models. We did not submit predictions using these models.

- **Disaster Tweets Classifier using BERTweet**: We train one additional model using the BERTweet model and achieve ~83% validation accuracy. This is a considerable improvement from the first set of models and ranked moderately high on the leaderboard (Best score: 0.83971; Rank: 93; Submission on: July 26th, 2023).
