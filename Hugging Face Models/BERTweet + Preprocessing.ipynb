{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Packages, Modules, and Data Sets","metadata":{}},{"cell_type":"code","source":"!pip install transformers --quiet\n!pip install evaluate --quiet","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:47:36.028018Z","iopub.execute_input":"2023-08-08T22:47:36.028414Z","iopub.status.idle":"2023-08-08T22:48:06.379657Z","shell.execute_reply.started":"2023-08-08T22:47:36.028383Z","shell.execute_reply":"2023-08-08T22:48:06.378147Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom emoji import demojize\nimport matplotlib.pyplot as plt\nimport os, re, random, datasets, evaluate\n\npd.set_option('display.max_colwidth', None)\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:09.302534Z","iopub.execute_input":"2023-08-08T22:48:09.302977Z","iopub.status.idle":"2023-08-08T22:48:29.566558Z","shell.execute_reply.started":"2023-08-08T22:48:09.302939Z","shell.execute_reply":"2023-08-08T22:48:29.565623Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:29.568183Z","iopub.execute_input":"2023-08-08T22:48:29.569191Z","iopub.status.idle":"2023-08-08T22:48:29.649250Z","shell.execute_reply.started":"2023-08-08T22:48:29.569155Z","shell.execute_reply":"2023-08-08T22:48:29.648079Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Pre-process Training and Test Data Sets","metadata":{}},{"cell_type":"code","source":"duplicates = train[train.duplicated('text')]\nproblematic_duplicates = []\n\nfor i in range(duplicates.text.nunique()):\n    duplicate_subset = train[train.text == duplicates.text.unique()[i]]\n    if len(duplicate_subset) > 1 and duplicate_subset.target.nunique() == 2:\n        problematic_duplicates.append(i)\n        \ntarget_list = [0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0]\n\nfor problematic_index in range(len(problematic_duplicates)): \n    train.target = np.where(train.text == duplicates.text.unique()[problematic_index], \n                            target_list[problematic_index], train.target)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:29.650619Z","iopub.execute_input":"2023-08-08T22:48:29.651023Z","iopub.status.idle":"2023-08-08T22:48:29.848150Z","shell.execute_reply.started":"2023-08-08T22:48:29.650989Z","shell.execute_reply":"2023-08-08T22:48:29.846962Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"The biggest change we are going to implement in this notebook is to perform additional pre-processing steps prior to fine-tuning the BERTweet model on Hugging Face. We are going to perform the same pre-processing steps that were performed prior to pre-training the BERTweet model as can be found in the [TweetNormalizer](https://github.com/VinAIResearch/BERTweet/blob/master/TweetNormalizer.py) module. Following are the steps that we are going to perform:\n\n- Lower case all characters;\n- Expand a few contractions (surprisingly few if you ask me. I would like to find out how they chose these contractions.)\n- Replace all account usernames (following '@') with \"USER\";\n- Replace all urls (following 'http' and/or 'www') with \"HTTPURL\";\n- Demojize (remove all emojis)","metadata":{}},{"cell_type":"code","source":"def clean_tweets(text):\n    \n    text = text.lower()\n    \n    text = text.replace(\"n't\", \" n't \")\n    text = text.replace(\"n 't\", \" n't \")\n    text = text.replace(\"ca n't\", \"can't\")\n    text = text.replace(\"ai n't\", \"ain't\")\n    \n    text = text.replace(\"'m\", \" 'm \")\n    text = text.replace(\"'re\", \" 're \")\n    text = text.replace(\"'s\", \" 's \")\n    text = text.replace(\"'ll\", \" 'll \")\n    text = text.replace(\"'d\", \" 'd \")\n    text = text.replace(\"'ve\", \" 've \")\n    text = text.replace(\"\\n\", \" \")\n    \n    text = text.replace(\" p . m .\", \" p.m.\")\n    text = text.replace(\" p . m \", \" p.m \")\n    text = text.replace(\" a . m .\", \" a.m.\")\n    text = text.replace(\" a . m \", \" a.m \")\n    \n    token_list = text.split(' ')\n    \n    token_list = [re.sub('#', '', x) for x in token_list]\n    token_list = [re.sub(r'@\\S+', '@USER', x) for x in token_list]\n    token_list = [re.sub(r'http\\S+', 'HTTPURL', x) for x in token_list]\n    token_list = [re.sub(r'www\\S+', 'HTTPURL', x) for x in token_list]\n    token_list = [demojize(x) if len(x) == 1 else x for x in token_list]\n    \n    return(\" \".join(token_list))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:29.851693Z","iopub.execute_input":"2023-08-08T22:48:29.852246Z","iopub.status.idle":"2023-08-08T22:48:29.865020Z","shell.execute_reply.started":"2023-08-08T22:48:29.852200Z","shell.execute_reply":"2023-08-08T22:48:29.863607Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"In case the \"location\" column provides additional information with regards to whether or not a tweet is about an actual disaster or not, let's append the substance of the location column to the text column. ","metadata":{}},{"cell_type":"code","source":"train.location = train.location.replace(np.nan, '', regex = True)\ntest_df.location = test_df.location.replace(np.nan, '', regex = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:29.867101Z","iopub.execute_input":"2023-08-08T22:48:29.867689Z","iopub.status.idle":"2023-08-08T22:48:29.885715Z","shell.execute_reply.started":"2023-08-08T22:48:29.867645Z","shell.execute_reply":"2023-08-08T22:48:29.884625Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train.text = train.text + \". \" + train.location + \".\"\ntest_df.text = test_df.text + \". \" + test_df.location + \".\"","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:29.887189Z","iopub.execute_input":"2023-08-08T22:48:29.887766Z","iopub.status.idle":"2023-08-08T22:48:29.906527Z","shell.execute_reply.started":"2023-08-08T22:48:29.887731Z","shell.execute_reply":"2023-08-08T22:48:29.905415Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train.text = train.text.apply(lambda x: clean_tweets(x))\ntest_df.text = test_df.text.apply(lambda x: clean_tweets(x))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:29.907784Z","iopub.execute_input":"2023-08-08T22:48:29.908160Z","iopub.status.idle":"2023-08-08T22:48:31.228617Z","shell.execute_reply.started":"2023-08-08T22:48:29.908131Z","shell.execute_reply":"2023-08-08T22:48:31.227183Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train[41:50]","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:31.230154Z","iopub.execute_input":"2023-08-08T22:48:31.230624Z","iopub.status.idle":"2023-08-08T22:48:31.254267Z","shell.execute_reply.started":"2023-08-08T22:48:31.230582Z","shell.execute_reply":"2023-08-08T22:48:31.252144Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"    id keyword                   location  \\\n41  61  ablaze                              \n42  62  ablaze                  milky way   \n43  63  ablaze                              \n44  64  ablaze                              \n45  65  ablaze                              \n46  66  ablaze  GREENSBORO,NORTH CAROLINA   \n47  67  ablaze                              \n48  68  ablaze             Live On Webcam   \n49  71  ablaze                   England.   \n\n                                                                                                                                                       text  \\\n41                                                                                     on the outside you 're  ablaze and alive but you 're  dead inside. .   \n42                     had an awesome time visiting the cfc head office the ancop site and ablaze. thanks to tita vida for taking care of us ??. milky way.   \n43                                                                                                                     soooo pumped for ablaze ???? @USER .   \n44                                                                          i wanted to set chicago ablaze with my preaching... but not my hotel! HTTPURL .   \n45                                                                      i gained 3 followers in the last week. you? know your stats and grow with HTTPURL .   \n46                                            how the west was burned: thousands of wildfires ablaze in california alone HTTPURL greensboro,north carolina.   \n47                                                                                       building the perfect tracklist to life leave the streets ablaze. .   \n48                                                                                   check these out: HTTPURL HTTPURL HTTPURL HTTPURL nsfw. live on webcam.   \n49  first night with retainers in. it 's  quite weird. better get used to it; i have to wear them every single night for the next year at least.. england..   \n\n    target  \n41       0  \n42       0  \n43       0  \n44       0  \n45       0  \n46       1  \n47       0  \n48       0  \n49       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>41</th>\n      <td>61</td>\n      <td>ablaze</td>\n      <td></td>\n      <td>on the outside you 're  ablaze and alive but you 're  dead inside. .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>62</td>\n      <td>ablaze</td>\n      <td>milky way</td>\n      <td>had an awesome time visiting the cfc head office the ancop site and ablaze. thanks to tita vida for taking care of us ??. milky way.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>63</td>\n      <td>ablaze</td>\n      <td></td>\n      <td>soooo pumped for ablaze ???? @USER .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>64</td>\n      <td>ablaze</td>\n      <td></td>\n      <td>i wanted to set chicago ablaze with my preaching... but not my hotel! HTTPURL .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>65</td>\n      <td>ablaze</td>\n      <td></td>\n      <td>i gained 3 followers in the last week. you? know your stats and grow with HTTPURL .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>66</td>\n      <td>ablaze</td>\n      <td>GREENSBORO,NORTH CAROLINA</td>\n      <td>how the west was burned: thousands of wildfires ablaze in california alone HTTPURL greensboro,north carolina.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>67</td>\n      <td>ablaze</td>\n      <td></td>\n      <td>building the perfect tracklist to life leave the streets ablaze. .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>68</td>\n      <td>ablaze</td>\n      <td>Live On Webcam</td>\n      <td>check these out: HTTPURL HTTPURL HTTPURL HTTPURL nsfw. live on webcam.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>71</td>\n      <td>ablaze</td>\n      <td>England.</td>\n      <td>first night with retainers in. it 's  quite weird. better get used to it; i have to wear them every single night for the next year at least.. england..</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train = train.groupby('target').sample(np.min(train.target.value_counts().to_list()), random_state = 1048597)\ntrain_df, val_df = np.split(train.sample(frac = 1), [int(0.8 * len(train))])","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:31.255578Z","iopub.execute_input":"2023-08-08T22:48:31.256138Z","iopub.status.idle":"2023-08-08T22:48:31.272844Z","shell.execute_reply.started":"2023-08-08T22:48:31.256103Z","shell.execute_reply":"2023-08-08T22:48:31.271542Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We are going to retain the columns that we are going to need for training and evaluation: *id* for test data set evaluation, *text* and *target* for both. Moving forward, I am going to stick to a pre-processing pipeline where we store the training, validation, and test data sets (if applicable) as Datasets inside one **DatasetDict**. ","metadata":{}},{"cell_type":"code","source":"train_df = train_df[['id', 'text', 'target']]\nval_df = val_df[['id', 'text', 'target']]\ntest_df = test_df[['id', 'text']]","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:54.203503Z","iopub.execute_input":"2023-08-08T22:48:54.203946Z","iopub.status.idle":"2023-08-08T22:48:54.216821Z","shell.execute_reply.started":"2023-08-08T22:48:54.203905Z","shell.execute_reply":"2023-08-08T22:48:54.215097Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_dict = datasets.Dataset.from_dict(train_df.to_dict(orient=\"list\"))\nval_dict = datasets.Dataset.from_dict(val_df.to_dict(orient=\"list\"))\ntest_dict = datasets.Dataset.from_dict(test_df.to_dict(orient=\"list\"))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:56.020342Z","iopub.execute_input":"2023-08-08T22:48:56.020845Z","iopub.status.idle":"2023-08-08T22:48:56.117750Z","shell.execute_reply.started":"2023-08-08T22:48:56.020804Z","shell.execute_reply":"2023-08-08T22:48:56.116334Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"tweets_ds = datasets.DatasetDict({\"train\": train_dict, \"val\": val_dict, \"test\": test_dict})","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:57.985630Z","iopub.execute_input":"2023-08-08T22:48:57.986092Z","iopub.status.idle":"2023-08-08T22:48:57.992424Z","shell.execute_reply.started":"2023-08-08T22:48:57.986056Z","shell.execute_reply":"2023-08-08T22:48:57.990763Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"tweets_ds","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:48:59.874744Z","iopub.execute_input":"2023-08-08T22:48:59.875229Z","iopub.status.idle":"2023-08-08T22:48:59.883463Z","shell.execute_reply.started":"2023-08-08T22:48:59.875193Z","shell.execute_reply":"2023-08-08T22:48:59.882298Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text', 'target'],\n        num_rows: 5209\n    })\n    val: Dataset({\n        features: ['id', 'text', 'target'],\n        num_rows: 1303\n    })\n    test: Dataset({\n        features: ['id', 'text'],\n        num_rows: 3263\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Finetune Hugging Face Model","metadata":{}},{"cell_type":"markdown","source":"We are going to use the **BERTweet-base** model on Hugging Face. The justification for this is the following: \n1. BERTweet has been trained on 850 million English Tweets. As we are trying to classify tweets, this model will capture the subtletie that only Tweets have;\n2. BERTweet has been trained based on the RoBERTa pre-training procedure. RoBERTa is generally a good model to fine-tune for classifcation purposes.","metadata":{}},{"cell_type":"code","source":"model_name = 'vinai/bertweet-base'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndata_collator = DataCollatorWithPadding(tokenizer = tokenizer)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:49:03.019065Z","iopub.execute_input":"2023-08-08T22:49:03.019587Z","iopub.status.idle":"2023-08-08T22:49:09.048025Z","shell.execute_reply.started":"2023-08-08T22:49:03.019544Z","shell.execute_reply":"2023-08-08T22:49:09.046835Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/558 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1f1c1b9cd8c409eb79bba79443c65c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6977b8a86aee48a5855e4a4248142b9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"addbc37d0c1946f093bb4388c0c8c68c"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5fbae968ca648f2a36836cb8b0cdccb"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_function(dataset):\n    return(tokenizer(dataset['text'], truncation = True))\n\ntokenized_data = tweets_ds.map(tokenize_function, batched = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:49:09.313826Z","iopub.execute_input":"2023-08-08T22:49:09.314480Z","iopub.status.idle":"2023-08-08T22:49:13.295652Z","shell.execute_reply.started":"2023-08-08T22:49:09.314445Z","shell.execute_reply":"2023-08-08T22:49:13.294302Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db841e25ea8a490caebd2b039de8a8b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16a26c00e86c49b6be2fc50493852474"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84870517b017466baf5ffbad90c0bcd2"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_data","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:49:14.771719Z","iopub.execute_input":"2023-08-08T22:49:14.772145Z","iopub.status.idle":"2023-08-08T22:49:14.779110Z","shell.execute_reply.started":"2023-08-08T22:49:14.772114Z","shell.execute_reply":"2023-08-08T22:49:14.777949Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text', 'target', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 5209\n    })\n    val: Dataset({\n        features: ['id', 'text', 'target', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1303\n    })\n    test: Dataset({\n        features: ['id', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 3263\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_data['train'] = tokenized_data['train'].rename_column('target', 'labels')\ntokenized_data['val'] = tokenized_data['val'].rename_column('target', 'labels')\ntokenized_data.with_format('pt')","metadata":{"execution":{"iopub.status.busy":"2023-08-08T22:49:17.017132Z","iopub.execute_input":"2023-08-08T22:49:17.017553Z","iopub.status.idle":"2023-08-08T22:49:17.034232Z","shell.execute_reply.started":"2023-08-08T22:49:17.017519Z","shell.execute_reply":"2023-08-08T22:49:17.032954Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 5209\n    })\n    val: Dataset({\n        features: ['id', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1303\n    })\n    test: Dataset({\n        features: ['id', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 3263\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"I came to learn that the column containing the labels (predictions) have to be named \"labels\". If not, **trainer.train()** will return an error. As the test dataset inside the DatasetDict object does not contain the labels yet, we are going to rename the \"target\" columns inside the train and validation datasets as \"labels\". ","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(model_name,  \n                                  evaluation_strategy = 'epoch',\n                                  num_train_epochs = 5,\n                                  learning_rate = 5e-5,\n                                  weight_decay = 0.005,\n                                  per_device_train_batch_size = 16,\n                                  per_device_eval_batch_size = 16,\n                                  report_to = 'none',\n                                  load_best_model_at_end = True,\n                                  save_strategy = 'epoch')\n\ndef compute_metrics(eval_pred):\n    metric = evaluate.load(\"accuracy\")\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis = -1)\n    return metric.compute(predictions=predictions, references=labels)\n\nearly_stop = EarlyStoppingCallback(2, 0.01)\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset = tokenized_data[\"train\"],\n    eval_dataset = tokenized_data[\"val\"],\n    data_collator = data_collator,\n    tokenizer = tokenizer,\n    compute_metrics = compute_metrics,\n    callbacks = [early_stop]\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T23:14:40.711012Z","iopub.execute_input":"2023-08-08T23:14:40.711447Z","iopub.status.idle":"2023-08-08T23:14:40.756159Z","shell.execute_reply.started":"2023-08-08T23:14:40.711414Z","shell.execute_reply":"2023-08-08T23:14:40.754926Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-08T23:14:47.052338Z","iopub.execute_input":"2023-08-08T23:14:47.053299Z","iopub.status.idle":"2023-08-09T00:20:52.233794Z","shell.execute_reply.started":"2023-08-08T23:14:47.053255Z","shell.execute_reply":"2023-08-09T00:20:52.232315Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='978' max='1630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 978/1630 1:06:00 < 44:05, 0.25 it/s, Epoch 3/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.481905</td>\n      <td>0.816577</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.357600</td>\n      <td>0.492348</td>\n      <td>0.802763</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.357600</td>\n      <td>0.635452</td>\n      <td>0.810437</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d105bd714962432590e78dd887e9eb0e"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=978, training_loss=0.31767982841756937, metrics={'train_runtime': 3964.7303, 'train_samples_per_second': 6.569, 'train_steps_per_second': 0.411, 'total_flos': 340291497339420.0, 'train_loss': 0.31767982841756937, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Prepare for Submission","metadata":{}},{"cell_type":"code","source":"test_predictions = trainer.predict(tokenized_data[\"test\"])\npreds = np.argmax(test_predictions.predictions, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T00:21:36.482296Z","iopub.execute_input":"2023-08-09T00:21:36.482779Z","iopub.status.idle":"2023-08-09T00:25:02.390767Z","shell.execute_reply.started":"2023-08-09T00:21:36.482745Z","shell.execute_reply":"2023-08-09T00:25:02.389332Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"submission = pd.DataFrame(list(zip(test_df.id, preds)), \n                          columns = [\"id\", \"target\"])\nsubmission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T00:25:02.393436Z","iopub.execute_input":"2023-08-09T00:25:02.393937Z","iopub.status.idle":"2023-08-09T00:25:02.434837Z","shell.execute_reply.started":"2023-08-09T00:25:02.393894Z","shell.execute_reply":"2023-08-09T00:25:02.433905Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Despite introducing an additional pre-processing step, the fine-tuned model did not perform better than [my best attempt](https://www.kaggle.com/code/l048596/disaster-tweets-bertweet-tensorflow). In following notebooks, I am going to learn to add additional layers to fine-tuned BERT models so that we can increase the expressive power of our models. It is my understanding that this can be done in one of two ways: 1) we define a custom module where we add layers on top of a pre-trained BERT model; 2) use input layers in tensorflow such that the input of the model are outputs of the pre-trained BERT model (similar to the one we have in the aforementioned notebook). ","metadata":{}}]}