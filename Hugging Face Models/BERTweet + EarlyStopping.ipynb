{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Objective of the Notebook","metadata":{}},{"cell_type":"markdown","source":"The objective of this notebook is to 1) build a classifier that does a good job of predicting whether or not a tweet is about an actual disaster or not (hopefully better than the classifiers that we've built previously); and 2) to establish a set of steps to fine-tune a model on Hugging Face for the purpose of classifying text. Hence, the steps that we take to fine-tune a model on Hugging Face is identical to the one that we use in [one of my previous notebooks that I use to classify contradictions](https://www.kaggle.com/code/l048596/contradiction-classifier-w-huggingface-87-41). I am doing two things differently: \n\n1. We are not \"cleaning\" the tweets **at all**. In the next iteration, I intend to perform the exact same pre-processing steps that were used to train BERTweet.\n2. I am introducing an EarlyStoppingCallback so that we can prevent the model from over-fitting on the training data set. ","metadata":{}},{"cell_type":"code","source":"!pip install transformers --quiet\n!pip install evaluate --quiet","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:45:01.594614Z","iopub.execute_input":"2023-08-07T23:45:01.594989Z","iopub.status.idle":"2023-08-07T23:45:22.068742Z","shell.execute_reply.started":"2023-08-07T23:45:01.594962Z","shell.execute_reply":"2023-08-07T23:45:22.067674Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os, re, random, datasets, evaluate\n\npd.set_option('display.max_colwidth', None)\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:45:22.071174Z","iopub.execute_input":"2023-08-07T23:45:22.071531Z","iopub.status.idle":"2023-08-07T23:45:38.858027Z","shell.execute_reply.started":"2023-08-07T23:45:22.071500Z","shell.execute_reply":"2023-08-07T23:45:38.856747Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:46:15.274765Z","iopub.execute_input":"2023-08-07T23:46:15.275110Z","iopub.status.idle":"2023-08-07T23:46:15.306414Z","shell.execute_reply.started":"2023-08-07T23:46:15.275081Z","shell.execute_reply":"2023-08-07T23:46:15.305075Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess the Training and Test Data Sets","metadata":{}},{"cell_type":"markdown","source":"Again, we are going to use the same set of pre-processing steps that we had used in previous notebooks covering this data set. We are going to identify duplicate observations with different labels, manually identify what the correct labels are, and assign the correct labels to them. Then, we are going to perform some pre-processing tasks to account for the fact that tweets contain urls, mentions, hashtags, and newlines.","metadata":{}},{"cell_type":"code","source":"duplicates = train[train.duplicated('text')]\nproblematic_duplicates = []\n\nfor i in range(duplicates.text.nunique()):\n    duplicate_subset = train[train.text == duplicates.text.unique()[i]]\n    if len(duplicate_subset) > 1 and duplicate_subset.target.nunique() == 2:\n        problematic_duplicates.append(i)\n\ntarget_list = [0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0]\n\nfor problematic_index in range(len(problematic_duplicates)): \n    train.target = np.where(train.text == duplicates.text.unique()[problematic_index], \n                            target_list[problematic_index], train.target)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:46:16.869392Z","iopub.execute_input":"2023-08-07T23:46:16.869775Z","iopub.status.idle":"2023-08-07T23:46:16.972109Z","shell.execute_reply.started":"2023-08-07T23:46:16.869744Z","shell.execute_reply":"2023-08-07T23:46:16.970954Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Split Data Sets into Train, Validation, and Test Data Sets","metadata":{}},{"cell_type":"code","source":"train = train.groupby('target').sample(3200, random_state = 1048596)\ntrain_df, val_df = np.split(train.sample(frac = 1), [int(0.8 * len(train))])","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:46:19.019561Z","iopub.execute_input":"2023-08-07T23:46:19.019964Z","iopub.status.idle":"2023-08-07T23:46:19.032492Z","shell.execute_reply.started":"2023-08-07T23:46:19.019929Z","shell.execute_reply":"2023-08-07T23:46:19.031472Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We are going to retain the columns that we are going to need for training and evaluation: *id* for test data set evaluation, *text* and *target* for both. Moving forward, I am going to stick to a pre-processing pipeline where we store the training, validation, and test data sets (if applicable) as Datasets inside one **DatasetDict**. ","metadata":{}},{"cell_type":"code","source":"train_df = train_df[['id', 'text', 'target']]\nval_df = val_df[['id', 'text', 'target']]\ntest_df = test_df[['id', 'text']]","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:46:21.169204Z","iopub.execute_input":"2023-08-07T23:46:21.169610Z","iopub.status.idle":"2023-08-07T23:46:21.182495Z","shell.execute_reply.started":"2023-08-07T23:46:21.169581Z","shell.execute_reply":"2023-08-07T23:46:21.180828Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_dict = datasets.Dataset.from_dict(train_df.to_dict(orient=\"list\"))\nval_dict = datasets.Dataset.from_dict(val_df.to_dict(orient=\"list\"))\ntest_dict = datasets.Dataset.from_dict(test_df.to_dict(orient=\"list\"))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:46:23.138892Z","iopub.execute_input":"2023-08-07T23:46:23.139407Z","iopub.status.idle":"2023-08-07T23:46:23.191319Z","shell.execute_reply.started":"2023-08-07T23:46:23.139380Z","shell.execute_reply":"2023-08-07T23:46:23.190304Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"tweets_ds = datasets.DatasetDict({\"train\": train_dict, \"val\": val_dict, \"test\": test_dict})","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:46:25.584965Z","iopub.execute_input":"2023-08-07T23:46:25.585321Z","iopub.status.idle":"2023-08-07T23:46:25.590243Z","shell.execute_reply.started":"2023-08-07T23:46:25.585293Z","shell.execute_reply":"2023-08-07T23:46:25.588719Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"tweets_ds","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:46:27.359805Z","iopub.execute_input":"2023-08-07T23:46:27.360143Z","iopub.status.idle":"2023-08-07T23:46:27.367311Z","shell.execute_reply.started":"2023-08-07T23:46:27.360115Z","shell.execute_reply":"2023-08-07T23:46:27.365925Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text', 'target'],\n        num_rows: 5120\n    })\n    val: Dataset({\n        features: ['id', 'text', 'target'],\n        num_rows: 1280\n    })\n    test: Dataset({\n        features: ['id', 'text'],\n        num_rows: 3263\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Finetune Hugging Face Model","metadata":{}},{"cell_type":"markdown","source":"We are going to use the **BERTweet-base** model on Hugging Face. The justification for this is the following: \n1. BERTweet has been trained on 850 million English Tweets. As we are trying to classify tweets, this model will capture the subtletie that only Tweets have;\n2. BERTweet has been trained based on the RoBERTa pre-training procedure. RoBERTa is generally a good model to fine-tune for classifcation purposes.","metadata":{}},{"cell_type":"code","source":"model_name = 'vinai/bertweet-base'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndata_collator = DataCollatorWithPadding(tokenizer = tokenizer)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:46:31.817452Z","iopub.execute_input":"2023-08-07T23:46:31.817785Z","iopub.status.idle":"2023-08-07T23:46:37.111758Z","shell.execute_reply.started":"2023-08-07T23:46:31.817758Z","shell.execute_reply":"2023-08-07T23:46:37.110374Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/558 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2378ddcb1bc4de49064b1000643e4f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81500b0620964596b512f6173cb2b5dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"405b8d8dbdb64d2fa82593d59149aa16"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7dd9628683b44609db2f784ceafdc17"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_function(dataset):\n    return(tokenizer(dataset['text'], truncation = True))\n\ntokenized_data = tweets_ds.map(tokenize_function, batched = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:46:39.388974Z","iopub.execute_input":"2023-08-07T23:46:39.389369Z","iopub.status.idle":"2023-08-07T23:46:43.070577Z","shell.execute_reply.started":"2023-08-07T23:46:39.389340Z","shell.execute_reply":"2023-08-07T23:46:43.069678Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20fd6d6c1a064731aa519b0dcc3c4ed3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"647dcf0db85f4fd691d01b8d992972af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e674fe8d89d543e18362adbc6167b2e9"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_data","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:46:44.236807Z","iopub.execute_input":"2023-08-07T23:46:44.237141Z","iopub.status.idle":"2023-08-07T23:46:44.242679Z","shell.execute_reply.started":"2023-08-07T23:46:44.237109Z","shell.execute_reply":"2023-08-07T23:46:44.241819Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text', 'target', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 5120\n    })\n    val: Dataset({\n        features: ['id', 'text', 'target', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1280\n    })\n    test: Dataset({\n        features: ['id', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 3263\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_data['train'] = tokenized_data['train'].rename_column('target', 'labels')\ntokenized_data['val'] = tokenized_data['val'].rename_column('target', 'labels')\ntokenized_data.with_format('pt')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:46:46.727858Z","iopub.execute_input":"2023-08-07T23:46:46.728533Z","iopub.status.idle":"2023-08-07T23:46:46.742600Z","shell.execute_reply.started":"2023-08-07T23:46:46.728499Z","shell.execute_reply":"2023-08-07T23:46:46.741324Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 5120\n    })\n    val: Dataset({\n        features: ['id', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1280\n    })\n    test: Dataset({\n        features: ['id', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 3263\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"I came to learn that the column containing the labels (predictions) have to be named \"labels\". If not, **trainer.train()** will return an error. As the test dataset inside the DatasetDict object does not contain the labels yet, we are going to rename the \"target\" columns inside the train and validation datasets as \"labels\". ","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(model_name,  \n                                  evaluation_strategy = 'epoch',\n                                  num_train_epochs = 5,\n                                  learning_rate = 5e-5,\n                                  weight_decay = 0.005,\n                                  per_device_train_batch_size = 16,\n                                  per_device_eval_batch_size = 16,\n                                  report_to = 'none',\n                                  load_best_model_at_end = True,\n                                  save_strategy = 'epoch')\n\ndef compute_metrics(eval_pred):\n    metric = evaluate.load(\"accuracy\")\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis = -1)\n    return metric.compute(predictions=predictions, references=labels)\n\nearly_stop = EarlyStoppingCallback(2, 0.01)\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset = tokenized_data[\"train\"],\n    eval_dataset = tokenized_data[\"val\"],\n    data_collator = data_collator,\n    tokenizer = tokenizer,\n    compute_metrics = compute_metrics,\n    callbacks = [early_stop]\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:48:38.183658Z","iopub.execute_input":"2023-08-07T23:48:38.184056Z","iopub.status.idle":"2023-08-07T23:48:38.200187Z","shell.execute_reply.started":"2023-08-07T23:48:38.184024Z","shell.execute_reply":"2023-08-07T23:48:38.199072Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:48:49.938898Z","iopub.execute_input":"2023-08-07T23:48:49.939413Z","iopub.status.idle":"2023-08-08T00:38:34.903212Z","shell.execute_reply.started":"2023-08-07T23:48:49.939369Z","shell.execute_reply":"2023-08-08T00:38:34.901680Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='960' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 960/1600 49:41 < 33:11, 0.32 it/s, Epoch 3/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.429196</td>\n      <td>0.828906</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.464900</td>\n      <td>0.432890</td>\n      <td>0.822656</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.464900</td>\n      <td>0.562370</td>\n      <td>0.806250</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5f719bd7dd14b9ba3707c852109be8e"}},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=960, training_loss=0.3944559574127197, metrics={'train_runtime': 2984.3629, 'train_samples_per_second': 8.578, 'train_steps_per_second': 0.536, 'total_flos': 361950367750080.0, 'train_loss': 0.3944559574127197, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Prepare for Submission","metadata":{}},{"cell_type":"code","source":"test_predictions = trainer.predict(tokenized_data[\"test\"])\npreds = np.argmax(test_predictions.predictions, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T00:38:34.905563Z","iopub.execute_input":"2023-08-08T00:38:34.906479Z","iopub.status.idle":"2023-08-08T00:41:26.897679Z","shell.execute_reply.started":"2023-08-08T00:38:34.906429Z","shell.execute_reply":"2023-08-08T00:41:26.896419Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"submission = pd.DataFrame(list(zip(test_df.id, preds)), \n                          columns = [\"id\", \"target\"])\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T00:41:26.899345Z","iopub.execute_input":"2023-08-08T00:41:26.899778Z","iopub.status.idle":"2023-08-08T00:41:26.932727Z","shell.execute_reply.started":"2023-08-08T00:41:26.899740Z","shell.execute_reply":"2023-08-08T00:41:26.931673Z"},"trusted":true},"execution_count":25,"outputs":[]}]}