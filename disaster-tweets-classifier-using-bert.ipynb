{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/l048596/disaster-tweets-classifier-using-bert?scriptVersionId=137860233\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install tensorflow --quiet\n!pip install tensorflow-hub --quiet\n!pip install tensorflow-text --quiet\n!pip install transformers --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-25T02:52:23.11454Z","iopub.execute_input":"2023-07-25T02:52:23.114889Z","iopub.status.idle":"2023-07-25T02:52:54.425743Z","shell.execute_reply.started":"2023-07-25T02:52:23.114858Z","shell.execute_reply":"2023-07-25T02:52:54.424552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, re, random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport matplotlib.pyplot as plt\n\ntf.get_logger().setLevel('ERROR')\npd.set_option('display.max_colwidth', None)\nos.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\"","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:52:55.55287Z","iopub.execute_input":"2023-07-25T02:52:55.553293Z","iopub.status.idle":"2023-07-25T02:53:38.763395Z","shell.execute_reply.started":"2023-07-25T02:52:55.553255Z","shell.execute_reply":"2023-07-25T02:53:38.76233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:53:38.765236Z","iopub.execute_input":"2023-07-25T02:53:38.765943Z","iopub.status.idle":"2023-07-25T02:53:38.831649Z","shell.execute_reply.started":"2023-07-25T02:53:38.765891Z","shell.execute_reply":"2023-07-25T02:53:38.830783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:53:38.832715Z","iopub.execute_input":"2023-07-25T02:53:38.833004Z","iopub.status.idle":"2023-07-25T02:53:38.843079Z","shell.execute_reply.started":"2023-07-25T02:53:38.832979Z","shell.execute_reply":"2023-07-25T02:53:38.842264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inspect Dataset for Mislabeled Tweets","metadata":{}},{"cell_type":"markdown","source":"In the discussion for this [competition](https://www.kaggle.com/competitions/nlp-getting-started/discussion/157982), I read that there are a lot of mislabeled tweets. Before we train models, let's check for mislabeled tweets and assign appropriate labels to them. As we cannot manually inspect all tweets, we are going to look for duplicate tweets and check that the duplicates have all been assigned the same labels. ","metadata":{}},{"cell_type":"code","source":"duplicates = train[train.duplicated('text')]\nduplicates.text.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:53:38.844755Z","iopub.execute_input":"2023-07-25T02:53:38.845119Z","iopub.status.idle":"2023-07-25T02:53:38.858174Z","shell.execute_reply.started":"2023-07-25T02:53:38.845092Z","shell.execute_reply":"2023-07-25T02:53:38.857166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are **69 duplicate tweets** inside the training dataset. We are going to iterate through these duplicate tweets to see if these duplicate tweets have unmatching labels. Unmatching labels would indicate that the tweet(s) has been mislabeled. We are going to store the index of these \"problematic duplicates\" inside a list and use it to iterate through these tweets so that we can re-assign correct labels after inspecting them.","metadata":{}},{"cell_type":"code","source":"problematic_duplicates = []\n\nfor i in range(duplicates.text.nunique()):\n    duplicate_subset = train[train.text == duplicates.text.unique()[i]]\n    if len(duplicate_subset) > 1 and duplicate_subset.target.nunique() == 2:\n        problematic_duplicates.append(i)\n        \nprint(problematic_duplicates)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:53:47.500589Z","iopub.execute_input":"2023-07-25T02:53:47.501343Z","iopub.status.idle":"2023-07-25T02:53:47.592325Z","shell.execute_reply.started":"2023-07-25T02:53:47.501305Z","shell.execute_reply":"2023-07-25T02:53:47.591436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train.text == duplicates.text.unique()[58]]","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:53:49.303747Z","iopub.execute_input":"2023-07-25T02:53:49.304616Z","iopub.status.idle":"2023-07-25T02:53:49.3203Z","shell.execute_reply.started":"2023-07-25T02:53:49.304579Z","shell.execute_reply":"2023-07-25T02:53:49.319348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above is the 58th duplicate. We see that these tweets have unmatching labels despite their texts being identical. This tweet is not about an actual disaster, so we are going to correctly assign both tweets as not being about an actual disaster. This is going to look like this: ","metadata":{}},{"cell_type":"code","source":"train.target = np.where(train.text == duplicates.text.unique()[58], 0, train.target)\ntrain[train.text == duplicates.text.unique()[58]]","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:53:51.08931Z","iopub.execute_input":"2023-07-25T02:53:51.089703Z","iopub.status.idle":"2023-07-25T02:53:51.104423Z","shell.execute_reply.started":"2023-07-25T02:53:51.089673Z","shell.execute_reply":"2023-07-25T02:53:51.103634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's repeat this step for all problematic duplicates after having identified the correct labels for each and every one of these problematic duplicates. We are going to store the correct labels inside a list and iterate through the problematic duplicates, assigning the correct labels one after the other.","metadata":{}},{"cell_type":"code","source":"target_list = [0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0]\n\nfor problematic_index in range(len(problematic_duplicates)): \n    train.target = np.where(train.text == duplicates.text.unique()[problematic_index], \n                            target_list[problematic_index], train.target)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:53:52.837785Z","iopub.execute_input":"2023-07-25T02:53:52.83821Z","iopub.status.idle":"2023-07-25T02:53:52.86575Z","shell.execute_reply.started":"2023-07-25T02:53:52.838175Z","shell.execute_reply":"2023-07-25T02:53:52.864731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-process the Tweets","metadata":{}},{"cell_type":"markdown","source":"Before we use the tweets to train the model, we are going to perform some basic pre-processing. To identify the appropriate steps, let's look at some of the tweets.","metadata":{}},{"cell_type":"code","source":"sample_train = train.sample(frac = 1, random_state = 1048596).head(5)\nsample_train","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:53:54.767031Z","iopub.execute_input":"2023-07-25T02:53:54.767413Z","iopub.status.idle":"2023-07-25T02:53:54.782356Z","shell.execute_reply.started":"2023-07-25T02:53:54.767383Z","shell.execute_reply":"2023-07-25T02:53:54.781357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the randomly selected tweets above, we see that the tweets contain links (http://...), hashtags (#..), and mentions (@..). We are going to remove links entirely and keep hashtags and mentions in case the words used in hashtags and mentions are useful for correctly classifying the tweets. We are going to define a function that performs the aforementioned pre-processing steps, in addition to lower-casing, removing numbers, removing indications of new lines (\\n), on the data frame and returns a cleaned data frame. ","metadata":{}},{"cell_type":"code","source":"def clean_text(dataframe):\n    dataframe.text = dataframe.text.apply(lambda x: str.lower(x))\n    dataframe.text = dataframe.text.apply(lambda x: re.sub(r'http\\S+', '', x))\n    dataframe.text = dataframe.text.apply(lambda x: re.sub(r'#', '', x))\n    dataframe.text = dataframe.text.apply(lambda x: re.sub(r'@', '', x))\n    dataframe.text = dataframe.text.apply(lambda x: re.sub(r'\\n', '', x))\n    dataframe.text = dataframe.text.apply(lambda x: re.sub(r'\\d+', '', x))\n    return(dataframe)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:53:58.264166Z","iopub.execute_input":"2023-07-25T02:53:58.264521Z","iopub.status.idle":"2023-07-25T02:53:58.27235Z","shell.execute_reply.started":"2023-07-25T02:53:58.264493Z","shell.execute_reply":"2023-07-25T02:53:58.271449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_train = clean_text(sample_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:54:00.199439Z","iopub.execute_input":"2023-07-25T02:54:00.199865Z","iopub.status.idle":"2023-07-25T02:54:00.208804Z","shell.execute_reply.started":"2023-07-25T02:54:00.199831Z","shell.execute_reply":"2023-07-25T02:54:00.207749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_train","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:54:01.40127Z","iopub.execute_input":"2023-07-25T02:54:01.402064Z","iopub.status.idle":"2023-07-25T02:54:01.41411Z","shell.execute_reply.started":"2023-07-25T02:54:01.402023Z","shell.execute_reply":"2023-07-25T02:54:01.413131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train = clean_text(train)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:54:12.89964Z","iopub.execute_input":"2023-07-25T02:54:12.900083Z","iopub.status.idle":"2023-07-25T02:54:13.0134Z","shell.execute_reply.started":"2023-07-25T02:54:12.900049Z","shell.execute_reply":"2023-07-25T02:54:13.012433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Balance the Training Dataset","metadata":{}},{"cell_type":"code","source":"clean_train.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:54:14.646706Z","iopub.execute_input":"2023-07-25T02:54:14.647086Z","iopub.status.idle":"2023-07-25T02:54:14.655409Z","shell.execute_reply.started":"2023-07-25T02:54:14.647056Z","shell.execute_reply":"2023-07-25T02:54:14.65447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_balanced = clean_train.groupby('target').sample(3000, random_state = 1048596)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:54:16.547319Z","iopub.execute_input":"2023-07-25T02:54:16.54769Z","iopub.status.idle":"2023-07-25T02:54:16.557967Z","shell.execute_reply.started":"2023-07-25T02:54:16.54766Z","shell.execute_reply":"2023-07-25T02:54:16.557103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = np.split(clean_train_balanced.sample(frac = 1), [int(0.8 * len(clean_train_balanced))])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:54:18.547241Z","iopub.execute_input":"2023-07-25T02:54:18.548107Z","iopub.status.idle":"2023-07-25T02:54:18.555591Z","shell.execute_reply.started":"2023-07-25T02:54:18.548071Z","shell.execute_reply":"2023-07-25T02:54:18.554746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of observations inside the training dataset: {}'.format(len(train_df)))\nprint('Number of observations inside the validation dataset: {}'.format(len(val_df)))","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:54:19.88499Z","iopub.execute_input":"2023-07-25T02:54:19.885381Z","iopub.status.idle":"2023-07-25T02:54:19.89088Z","shell.execute_reply.started":"2023-07-25T02:54:19.885352Z","shell.execute_reply":"2023-07-25T02:54:19.889943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((train_df.text, train_df.target)).shuffle(1000).batch(32)\nval_dataset = tf.data.Dataset.from_tensor_slices((val_df.text, val_df.target)).shuffle(1000).batch(32)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:54:21.458408Z","iopub.execute_input":"2023-07-25T02:54:21.459077Z","iopub.status.idle":"2023-07-25T02:54:25.163195Z","shell.execute_reply.started":"2023-07-25T02:54:21.45904Z","shell.execute_reply":"2023-07-25T02:54:25.161959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define and Train Model - First Model","metadata":{}},{"cell_type":"markdown","source":"In our first model, we are going to fine-tune BERT. A tutorial for fine-tuning BERT for text classification can be found in [this tensorflow tutorial page](https://www.tensorflow.org/text/tutorials/classify_text_with_bert). Similar to the model introduced in the tutorial, we are going to use a preprocessing layer, a 'bert-base-uncased' model, a Dropout layer, and two Dense layers. ","metadata":{}},{"cell_type":"code","source":"tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\ntfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'","metadata":{"execution":{"iopub.status.busy":"2023-07-25T01:28:29.848572Z","iopub.execute_input":"2023-07-25T01:28:29.849562Z","iopub.status.idle":"2023-07-25T01:28:29.85352Z","shell.execute_reply.started":"2023-07-25T01:28:29.849524Z","shell.execute_reply":"2023-07-25T01:28:29.852438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_preprocess = hub.KerasLayer(tfhub_handle_preprocess)\nbert_encoder = hub.KerasLayer(tfhub_handle_encoder)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T01:28:33.983996Z","iopub.execute_input":"2023-07-25T01:28:33.984393Z","iopub.status.idle":"2023-07-25T01:28:56.27713Z","shell.execute_reply.started":"2023-07-25T01:28:33.984363Z","shell.execute_reply":"2023-07-25T01:28:56.275678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_input = tf.keras.layers.Input(shape = (), dtype = tf.string)\nencoder_input = bert_preprocess(text_input)\nencoder_output = bert_encoder(encoder_input)\n    \nl = tf.keras.layers.Dropout(0.3)(encoder_output['pooled_output'])\nl = tf.keras.layers.Dense(16, activation = 'relu')(l)\nl = tf.keras.layers.Dense(1, activation = 'sigmoid')(l)\n    \nmodel = tf.keras.Model(inputs=[text_input], outputs = [l])\nmodel.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n              loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T01:59:12.610974Z","iopub.execute_input":"2023-07-25T01:59:12.611409Z","iopub.status.idle":"2023-07-25T01:59:12.83419Z","shell.execute_reply.started":"2023-07-25T01:59:12.611377Z","shell.execute_reply":"2023-07-25T01:59:12.83268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are going to introduce a callback so that the model stops training when validation loss stops to improve for two epochs in a row. That way, when the model starts to over-fit and fails to generalize to the validation dataset, we can stop the model from training. ","metadata":{}},{"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n                                                  patience = 2)\n\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath = 'model/best_performed_model.ckpt',\n    save_weights_only = True,\n    save_best_only = True,\n    monitor = 'val_loss',\n    verbose = 1\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:55:14.21594Z","iopub.execute_input":"2023-07-25T02:55:14.216343Z","iopub.status.idle":"2023-07-25T02:55:14.222053Z","shell.execute_reply.started":"2023-07-25T02:55:14.21631Z","shell.execute_reply":"2023-07-25T02:55:14.221079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    validation_data = val_dataset,\n                    epochs = 30, \n                    callbacks = [early_stopping, model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T01:59:16.319419Z","iopub.execute_input":"2023-07-25T01:59:16.320386Z","iopub.status.idle":"2023-07-25T02:24:47.969571Z","shell.execute_reply.started":"2023-07-25T01:59:16.320341Z","shell.execute_reply":"2023-07-25T02:24:47.968033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['training', 'validation'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:25:19.182119Z","iopub.execute_input":"2023-07-25T02:25:19.183249Z","iopub.status.idle":"2023-07-25T02:25:19.434105Z","shell.execute_reply.started":"2023-07-25T02:25:19.183211Z","shell.execute_reply":"2023-07-25T02:25:19.432729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot suggested that the model stopped training before over-fitting on the training data. This first model (BERT model) returned a validation accuracy of .70. ","metadata":{}},{"cell_type":"markdown","source":"## Define and Train Model - Second Model","metadata":{}},{"cell_type":"markdown","source":"In the second model, we are going use a preprocessing layer (the same one as the previous one), a 'Small BERT' model, a Dropout layer, and two Dense layers. The Small BERT is smaller in size and thus more efficient for downstream tasks such as text classification. ","metadata":{}},{"cell_type":"code","source":"tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\ntfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1'","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:54:39.383607Z","iopub.execute_input":"2023-07-25T02:54:39.384Z","iopub.status.idle":"2023-07-25T02:54:39.388349Z","shell.execute_reply.started":"2023-07-25T02:54:39.383969Z","shell.execute_reply":"2023-07-25T02:54:39.38741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_preprocess = hub.KerasLayer(tfhub_handle_preprocess)\nbert_encoder = hub.KerasLayer(tfhub_handle_encoder)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:54:41.431054Z","iopub.execute_input":"2023-07-25T02:54:41.431451Z","iopub.status.idle":"2023-07-25T02:54:51.791529Z","shell.execute_reply.started":"2023-07-25T02:54:41.431419Z","shell.execute_reply":"2023-07-25T02:54:51.790392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_input = tf.keras.layers.Input(shape = (), dtype = tf.string)\nencoder_input = bert_preprocess(text_input)\nencoder_output = bert_encoder(encoder_input)\n\nl = tf.keras.layers.Dense(32, activation = 'relu')(encoder_output['pooled_output'])\nl = tf.keras.layers.Dropout(0.3)(l)\nl = tf.keras.layers.Dense(1, activation = 'sigmoid')(l)\n\nmodel = tf.keras.Model(inputs=[text_input], outputs = [l])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:31:22.726579Z","iopub.execute_input":"2023-07-25T02:31:22.728028Z","iopub.status.idle":"2023-07-25T02:31:22.88512Z","shell.execute_reply.started":"2023-07-25T02:31:22.727983Z","shell.execute_reply":"2023-07-25T02:31:22.883685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n              loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:31:35.024588Z","iopub.execute_input":"2023-07-25T02:31:35.025037Z","iopub.status.idle":"2023-07-25T02:31:35.038726Z","shell.execute_reply.started":"2023-07-25T02:31:35.025004Z","shell.execute_reply":"2023-07-25T02:31:35.037439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    validation_data = val_dataset,\n                    epochs = 30, \n                    callbacks = [early_stopping, model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:31:46.308804Z","iopub.execute_input":"2023-07-25T02:31:46.309263Z","iopub.status.idle":"2023-07-25T02:34:32.310665Z","shell.execute_reply.started":"2023-07-25T02:31:46.309227Z","shell.execute_reply":"2023-07-25T02:34:32.309339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['training', 'validation'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:39:37.172126Z","iopub.execute_input":"2023-07-25T02:39:37.173273Z","iopub.status.idle":"2023-07-25T02:39:37.388995Z","shell.execute_reply.started":"2023-07-25T02:39:37.173234Z","shell.execute_reply":"2023-07-25T02:39:37.387951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The second model returned a validation accuracy of .78. This was a considerable improvement from the first model. The model stopped training although there was no indication of over-fitting, but both training and validation accuracy values were plateauing by the end of the fifth epoch. ","metadata":{}},{"cell_type":"markdown","source":"## Define and Train Model - Second Model (2)","metadata":{}},{"cell_type":"code","source":"text_input = tf.keras.layers.Input(shape = (), dtype = tf.string)\nencoder_input = bert_preprocess(text_input)\nencoder_output = bert_encoder(encoder_input)\n\nl = tf.keras.layers.Dense(32, activation = 'relu')(encoder_output['pooled_output'])\nl = tf.keras.layers.Dropout(0.3)(l)\nl = tf.keras.layers.Dense(16, activation = 'relu')(l)\nl = tf.keras.layers.Dropout(0.3)(l)\nl = tf.keras.layers.Dense(1, activation = 'sigmoid')(l)\n\nmodel = tf.keras.Model(inputs=[text_input], outputs = [l])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:54:51.793358Z","iopub.execute_input":"2023-07-25T02:54:51.793822Z","iopub.status.idle":"2023-07-25T02:54:52.553375Z","shell.execute_reply.started":"2023-07-25T02:54:51.793787Z","shell.execute_reply":"2023-07-25T02:54:52.552215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n              loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:55:04.797503Z","iopub.execute_input":"2023-07-25T02:55:04.79788Z","iopub.status.idle":"2023-07-25T02:55:05.061706Z","shell.execute_reply.started":"2023-07-25T02:55:04.797849Z","shell.execute_reply":"2023-07-25T02:55:05.060724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    validation_data = val_dataset,\n                    epochs = 30, \n                    callbacks = [early_stopping, model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:55:18.668869Z","iopub.execute_input":"2023-07-25T02:55:18.669575Z","iopub.status.idle":"2023-07-25T02:57:45.070263Z","shell.execute_reply.started":"2023-07-25T02:55:18.66953Z","shell.execute_reply":"2023-07-25T02:57:45.068967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['training', 'validation'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:58:30.684247Z","iopub.execute_input":"2023-07-25T02:58:30.68473Z","iopub.status.idle":"2023-07-25T02:58:31.023099Z","shell.execute_reply.started":"2023-07-25T02:58:30.684692Z","shell.execute_reply":"2023-07-25T02:58:31.021951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The updated model returned a validation accuracy of .76. This was a regression in model performance from the previous one, and validation accuracy started to decrease from the second epoch while training accuracy continued to increase. This is indication that the model started over-fitting since the second epoch. Adding complexity was not the right way to go. ","metadata":{}},{"cell_type":"markdown","source":"## Define and Train Model - Third Model","metadata":{}},{"cell_type":"markdown","source":"As the second model using the 'Small BERT' model performed considerably better than the first model using the 'bert-based-uncased' model, we are going to continue using the 'Small BERT' model for the new models that follow. This time, we are going to feed the sequential output of the BERT layer into Bidirectional LSTMs instead of immediately feeding the BERT embeddings to a Dense layer as we did in the first two models. ","metadata":{}},{"cell_type":"code","source":"text_input = tf.keras.layers.Input(shape = (), dtype = tf.string)\nencoder_input = bert_preprocess(text_input)\nencoder_output = bert_encoder(encoder_input)\n\nl = tf.keras.layers.Dense(200, activation = 'relu')(encoder_output['sequence_output'])\nl = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200, return_sequences = False, dropout = 0.3))(l)\nl = tf.keras.layers.Dense(50, activation = 'relu')(l)\nl = tf.keras.layers.Dropout(0.3)(l)\nl = tf.keras.layers.Dense(1, activation = 'sigmoid')(l)\n\nmodel = tf.keras.Model(inputs=[text_input], outputs = [l])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T03:11:45.179699Z","iopub.execute_input":"2023-07-25T03:11:45.180882Z","iopub.status.idle":"2023-07-25T03:11:45.821361Z","shell.execute_reply.started":"2023-07-25T03:11:45.180828Z","shell.execute_reply":"2023-07-25T03:11:45.81997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n              loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T03:11:47.029009Z","iopub.execute_input":"2023-07-25T03:11:47.029411Z","iopub.status.idle":"2023-07-25T03:11:47.042548Z","shell.execute_reply.started":"2023-07-25T03:11:47.029374Z","shell.execute_reply":"2023-07-25T03:11:47.041341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    validation_data = val_dataset,\n                    epochs = 30, \n                    callbacks = [early_stopping, model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T03:11:48.331114Z","iopub.execute_input":"2023-07-25T03:11:48.331563Z","iopub.status.idle":"2023-07-25T03:17:55.422701Z","shell.execute_reply.started":"2023-07-25T03:11:48.331525Z","shell.execute_reply":"2023-07-25T03:17:55.420593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['training', 'validation'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T03:18:47.376851Z","iopub.execute_input":"2023-07-25T03:18:47.377675Z","iopub.status.idle":"2023-07-25T03:18:47.641996Z","shell.execute_reply.started":"2023-07-25T03:18:47.377636Z","shell.execute_reply":"2023-07-25T03:18:47.640366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The third model returned a validation accuracy of .78. This was a considerable improvement from the first model but not from the second model. We are going to resume working on this task in a new notebook. ","metadata":{}},{"cell_type":"markdown","source":"## Prepare for Submission","metadata":{}},{"cell_type":"code","source":"cleaned_test = clean_text(test)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T03:20:57.180716Z","iopub.execute_input":"2023-07-25T03:20:57.181648Z","iopub.status.idle":"2023-07-25T03:20:57.236803Z","shell.execute_reply.started":"2023-07-25T03:20:57.181606Z","shell.execute_reply":"2023-07-25T03:20:57.235416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(cleaned_test.text)\npred_list = [np.mean(x) for x in predictions]","metadata":{"execution":{"iopub.status.busy":"2023-07-25T03:21:43.820718Z","iopub.execute_input":"2023-07-25T03:21:43.821106Z","iopub.status.idle":"2023-07-25T03:22:09.92332Z","shell.execute_reply.started":"2023-07-25T03:21:43.821074Z","shell.execute_reply":"2023-07-25T03:22:09.922047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = [(x > 0.5).astype(int) for x in pred_list]","metadata":{"execution":{"iopub.status.busy":"2023-07-25T03:23:10.068696Z","iopub.execute_input":"2023-07-25T03:23:10.06947Z","iopub.status.idle":"2023-07-25T03:23:10.083332Z","shell.execute_reply.started":"2023-07-25T03:23:10.069433Z","shell.execute_reply":"2023-07-25T03:23:10.082169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df = pd.DataFrame(list(zip(test.id, final_predictions)),\n                              columns = ['id', 'target'])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T03:23:11.648001Z","iopub.execute_input":"2023-07-25T03:23:11.648845Z","iopub.status.idle":"2023-07-25T03:23:11.665656Z","shell.execute_reply.started":"2023-07-25T03:23:11.64881Z","shell.execute_reply":"2023-07-25T03:23:11.664402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df.to_csv('predictions.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T03:23:13.223041Z","iopub.execute_input":"2023-07-25T03:23:13.224015Z","iopub.status.idle":"2023-07-25T03:23:13.237853Z","shell.execute_reply.started":"2023-07-25T03:23:13.223974Z","shell.execute_reply":"2023-07-25T03:23:13.236762Z"},"trusted":true},"execution_count":null,"outputs":[]}]}